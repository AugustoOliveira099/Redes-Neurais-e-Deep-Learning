{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmYGVziz50tu"
      },
      "source": [
        "- Remove non alphanumeric characters for simple training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOwRggVcwtzP"
      },
      "outputs": [],
      "source": [
        "# from transformer import Transformer # this is the transformer.py file\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mRzIDd6r_U6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
        "        scaled = scaled.permute(1, 0, 2, 3)\n",
        "    attention = F.softmax(scaled, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_sequence_length):\n",
        "        super().__init__()\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self):\n",
        "        even_i = torch.arange(0, self.d_model, 2).float()\n",
        "        denominator = torch.pow(10000, even_i/self.d_model)\n",
        "        position = (torch.arange(self.max_sequence_length)\n",
        "                          .reshape(self.max_sequence_length, 1))\n",
        "        even_PE = torch.sin(position / denominator)\n",
        "        odd_PE = torch.cos(position / denominator)\n",
        "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
        "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
        "        return PE\n",
        "\n",
        "class SentenceEmbedding(nn.Module):\n",
        "    \"For a given sentence, create an embedding\"\n",
        "    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.vocab_size = len(language_to_index)\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.embedding = nn.Embedding(self.vocab_size, d_model)\n",
        "        self.language_to_index = language_to_index\n",
        "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.START_TOKEN = START_TOKEN\n",
        "        self.END_TOKEN = END_TOKEN\n",
        "        self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "    def batch_tokenize(self, batch, start_token, end_token):\n",
        "\n",
        "        def tokenize(sentence, start_token, end_token):\n",
        "            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]\n",
        "            if start_token:\n",
        "                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])\n",
        "            if end_token:\n",
        "                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])\n",
        "            for _ in range(len(sentence_word_indicies), self.max_sequence_length):\n",
        "                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])\n",
        "            return torch.tensor(sentence_word_indicies)\n",
        "\n",
        "        tokenized = []\n",
        "        for sentence_num in range(len(batch)):\n",
        "           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )\n",
        "        tokenized = torch.stack(tokenized)\n",
        "        return tokenized.to(get_device())\n",
        "\n",
        "    def forward(self, x, start_token, end_token): # sentence\n",
        "        x = self.batch_tokenize(x, start_token, end_token)\n",
        "        x = self.embedding(x)\n",
        "        pos = self.position_encoder().to(get_device())\n",
        "        x = self.dropout(x + pos)\n",
        "        return x\n",
        "\n",
        "\n",
        "class  .(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, sequence_length, d_model = x.size()\n",
        "        qkv = self.qkv_layer(x)\n",
        "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3)\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, parameters_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.parameters_shape=parameters_shape\n",
        "        self.eps=eps\n",
        "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
        "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
        "        mean = inputs.mean(dim=dims, keepdim=True)\n",
        "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
        "        std = (var + self.eps).sqrt()\n",
        "        y = (inputs - mean) / std\n",
        "        out = self.gamma * y + self.beta\n",
        "        return out\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden)\n",
        "        self.linear2 = nn.Linear(hidden, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, self_attention_mask):\n",
        "        residual_x = x.clone()\n",
        "        x = self.attention(x, mask=self_attention_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + residual_x)\n",
        "        residual_x = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + residual_x)\n",
        "        return x\n",
        "\n",
        "class SequentialEncoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, self_attention_mask  = inputs\n",
        "        for module in self._modules.values():\n",
        "            x = module(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, self_attention_mask, start_token, end_token):\n",
        "        x = self.sentence_embedding(x, start_token, end_token)\n",
        "        x = self.layers(x, self_attention_mask)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiHeadCrossAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
        "        self.q_layer = nn.Linear(d_model , d_model)\n",
        "        self.linear_layer = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x, y, mask):\n",
        "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
        "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(0, 2, 1, 3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k, v = kv.chunk(2, dim=-1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
        "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
        "        out = self.linear_layer(values)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
        "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
        "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
        "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
        "        _y = y.clone()\n",
        "        y = self.self_attention(y, mask=self_attention_mask)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.layer_norm1(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.layer_norm2(y + _y)\n",
        "\n",
        "        _y = y.clone()\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.layer_norm3(y + _y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SequentialDecoder(nn.Sequential):\n",
        "    def forward(self, *inputs):\n",
        "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
        "        for module in self._modules.values():\n",
        "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 ffn_hidden,\n",
        "                 num_heads,\n",
        "                 drop_prob,\n",
        "                 num_layers,\n",
        "                 max_sequence_length,\n",
        "                 language_to_index,\n",
        "                 START_TOKEN,\n",
        "                 END_TOKEN,\n",
        "                 PADDING_TOKEN):\n",
        "        super().__init__()\n",
        "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):\n",
        "        y = self.sentence_embedding(y, start_token, end_token)\n",
        "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                d_model,\n",
        "                ffn_hidden,\n",
        "                num_heads,\n",
        "                drop_prob,\n",
        "                num_layers,\n",
        "                max_sequence_length,\n",
        "                kn_vocab_size,\n",
        "                english_to_index,\n",
        "                kannada_to_index,\n",
        "                START_TOKEN,\n",
        "                END_TOKEN,\n",
        "                PADDING_TOKEN\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
        "        self.linear = nn.Linear(d_model, kn_vocab_size)\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                y,\n",
        "                encoder_self_attention_mask=None,\n",
        "                decoder_self_attention_mask=None,\n",
        "                decoder_cross_attention_mask=None,\n",
        "                enc_start_token=False,\n",
        "                enc_end_token=False,\n",
        "                dec_start_token=False, # We should make this true\n",
        "                dec_end_token=False): # x, y are batch of sentences\n",
        "        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)\n",
        "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TApzOj5xCwR"
      },
      "outputs": [],
      "source": [
        "# Generated this by filtering Appendix code\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "PADDING_TOKEN = '<PADDING>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "kannada_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '<', '=', '>', '?', 'ˌ',\n",
        "                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ',\n",
        "                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ',\n",
        "                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ',\n",
        "                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ',\n",
        "                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ',\n",
        "                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ',\n",
        "                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ',\n",
        "                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ',\n",
        "                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ',\n",
        "                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]\n",
        "\n",
        "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
        "                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "                        ':', '<', '=', '>', '?', '@',\n",
        "                        '[', '\\\\', ']', '^', '_', '`',\n",
        "                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
        "                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x',\n",
        "                        'y', 'z',\n",
        "                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA8ESmCrNoc7"
      },
      "outputs": [],
      "source": [
        "index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}\n",
        "kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}\n",
        "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
        "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SYGjRdoxRg-",
        "outputId": "a4486422-59a3-4ed3-815f-411b976f9ba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "english_file = '/content/drive/My Drive/transformer/dataset/english.txt' # replace this path with appropriate one\n",
        "kannada_file = '/content/drive/My Drive/transformer/dataset/kannada.txt' # replace this path with appropriate one\n",
        "\n",
        "with open(english_file, 'r') as file:\n",
        "    english_sentences = file.readlines()\n",
        "with open(kannada_file, 'r') as file:\n",
        "    kannada_sentences = file.readlines()\n",
        "\n",
        "# Limit Number of sentences\n",
        "TOTAL_SENTENCES = 200000\n",
        "english_sentences = english_sentences[:TOTAL_SENTENCES]\n",
        "kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]\n",
        "english_sentences = [sentence.rstrip('\\n').lower() for sentence in english_sentences]\n",
        "kannada_sentences = [sentence.rstrip('\\n') for sentence in kannada_sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUB-BkgFxXfM",
        "outputId": "ea4cde90-3cd2-499c-cbe4-e29ef9789278"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hes a scientist.',\n",
              " \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
              " '8 lakh crore have been looted.',\n",
              " 'i read a lot into this as well.',\n",
              " \"she was found dead with the phone's battery exploded close to her head the following morning.\",\n",
              " 'how did mankind come under satans rival sovereignty?',\n",
              " 'and then i became prime minister.',\n",
              " 'what about corruption?',\n",
              " 'no differences',\n",
              " '\"\"\"the shooting of the film is 90 percent done.\"']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OT-aznAxc5U",
        "outputId": "fbc49892-b180-49fa-d8a9-e4d47468a08d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.',\n",
              " 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.',\n",
              " 'ಆಕೆಯ ತಲೆಯ ಹತ್ತಿರ ಇರಿಸಿಕೊಂಡಿದ್ದ ಫೋನ್\\u200cನ ಬ್ಯಾಟರಿ ಸ್ಫೋಟಗೊಂಡು ಆಕೆ ಮೃತಪಟ್ಟಿದ್ದಾಳೆ ಎನ್ನಲಾಗಿದೆ.',\n",
              " 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?',\n",
              " 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.',\n",
              " 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?',\n",
              " '‘ಅನುಪಾತದಲ್ಲಿ ವ್ಯತ್ಯಾಸವಿಲ್ಲ’',\n",
              " 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8VAutsTxlaR",
        "outputId": "7a78445c-52b5-4e3e-e50f-cd5d62273c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "97th percentile length Kannada: 172.0\n",
            "97th percentile length English: 178.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "PERCENTILE = 97\n",
        "print( f\"{PERCENTILE}th percentile length Kannada: {np.percentile([len(x) for x in kannada_sentences], PERCENTILE)}\" )\n",
        "print( f\"{PERCENTILE}th percentile length English: {np.percentile([len(x) for x in english_sentences], PERCENTILE)}\" )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG9ezqvaxl4b",
        "outputId": "a5968fae-bc1b-4c70-f3f5-03e6a438d139"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of sentences: 200000\n",
            "Number of valid sentences: 164022\n"
          ]
        }
      ],
      "source": [
        "max_sequence_length = 200\n",
        "\n",
        "def is_valid_tokens(sentence, vocab):\n",
        "    for token in list(set(sentence)):\n",
        "        if token not in vocab:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_valid_length(sentence, max_sequence_length):\n",
        "    return len(list(sentence)) < (max_sequence_length - 1) # need to re-add the end token so leaving 1 space\n",
        "\n",
        "valid_sentence_indicies = []\n",
        "for index in range(len(kannada_sentences)):\n",
        "    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]\n",
        "    if is_valid_length(kannada_sentence, max_sequence_length) \\\n",
        "      and is_valid_length(english_sentence, max_sequence_length) \\\n",
        "      and is_valid_tokens(kannada_sentence, kannada_vocabulary):\n",
        "        valid_sentence_indicies.append(index)\n",
        "\n",
        "print(f\"Number of sentences: {len(kannada_sentences)}\")\n",
        "print(f\"Number of valid sentences: {len(valid_sentence_indicies)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o80QDn4CxsV7"
      },
      "outputs": [],
      "source": [
        "kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]\n",
        "english_sentences = [english_sentences[i] for i in valid_sentence_indicies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35xhLztQiLIQ",
        "outputId": "b1ad2be6-6c28-46b9-d943-0f8cec377912"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.',\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"',\n",
              " 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kannada_sentences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqOFnclmyxAE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "d_model = 512\n",
        "batch_size = 30\n",
        "ffn_hidden = 2048\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "num_layers = 1\n",
        "max_sequence_length = 200\n",
        "kn_vocab_size = len(kannada_vocabulary)\n",
        "\n",
        "transformer = Transformer(d_model,\n",
        "                          ffn_hidden,\n",
        "                          num_heads,\n",
        "                          drop_prob,\n",
        "                          num_layers,\n",
        "                          max_sequence_length,\n",
        "                          kn_vocab_size,\n",
        "                          english_to_index,\n",
        "                          kannada_to_index,\n",
        "                          START_TOKEN,\n",
        "                          END_TOKEN,\n",
        "                          PADDING_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc2hYQk9yxX0",
        "outputId": "e2585654-de0b-48e2-c8b2-3376e3a76805"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(71, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialEncoder(\n",
              "      (0): EncoderLayer(\n",
              "        (attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (sentence_embedding): SentenceEmbedding(\n",
              "      (embedding): Embedding(125, 512)\n",
              "      (position_encoder): PositionalEncoding()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (layers): SequentialDecoder(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attention): MultiHeadAttention(\n",
              "          (qkv_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNormalization()\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (encoder_decoder_attention): MultiHeadCrossAttention(\n",
              "          (kv_layer): Linear(in_features=512, out_features=1024, bias=True)\n",
              "          (q_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear_layer): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNormalization()\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        (ffn): PositionwiseFeedForward(\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (relu): ReLU()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (layer_norm3): LayerNormalization()\n",
              "        (dropout3): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=125, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asUJX-STy7fg"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, english_sentences, kannada_sentences):\n",
        "        self.english_sentences = english_sentences\n",
        "        self.kannada_sentences = kannada_sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.english_sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.english_sentences[idx], self.kannada_sentences[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-auNWjkdzDge"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(english_sentences, kannada_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roH2A4m4zF4z",
        "outputId": "75cda49e-f9d7-4df3-83ac-dc0b5a6ef1fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "164022"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGeHNlzozIGF",
        "outputId": "e42d4cbd-13b7-4339-eb7b-892d526c0514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(\"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\",\n",
              " '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"')"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YDttjQ0zMrv"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset, batch_size)\n",
        "iterator = iter(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EnjHKB1zM8Y",
        "outputId": "2482ae10-0340-42f1-86db-660399c2ae8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('hes a scientist.', \"'but we speak the truth aur ye sach hai ke gujarat mein vikas pagal hogaya hai,'' rahul gandhi further said in banaskantha\", '8 lakh crore have been looted.', 'i read a lot into this as well.', 'how did mankind come under satans rival sovereignty?', 'and then i became prime minister.', 'what about corruption?', '\"\"\"the shooting of the film is 90 percent done.\"', 'the special statute', '\"then the king said to ittai the gittite, \"\"why do you also go with us? return, and stay with the king. for you are a foreigner, and also an exile. return to your own place.\"', 'what happened at the un general assembly?', 'the meeting was attended by prime minister narendra modi, home minister amit shah and defence minister rajnath singh, among others.', 'it has been under discussion for a long time.', 'buses cannot get there.', 'why then this tradition was not thought of?', 'kashmiri youth join indian army', 'basic amenities elude this village', 'off-budget borrowings of the state increased from rs853 crore in 2011-12 to rs,173 crore in 2017-18', 'during the monsoon season, the rubbish is swept on to the road by rainwater, creating problems for the traffic.', 'however, the other two, ramesh jarkiholi and mahesh kumatahalli, had not given any reason, he said.', 'how to make pasta salad?', 'he accused the modi government of ruining the countrys economy.', 'add chopped carrots and potatoes.', 'first shot', 'after the incident, police reached the spot and admitted the injured to the hospital.', 'her father gave kunti to his childless cousin kuntibhoja.', 'how to eat', 'granted, the standard of living varies from country to country.', 'for example, medical.', 'thats a fine plan.'), ('ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.', '\"ಆದರೆ ಸತ್ಯ ಹೊರ ಬಂದೇ ಬರುತ್ತದೆ ಎಂದು ಹೇಳಿದ ರಾಹುಲ್ ಗಾಂಧಿ, \"\"ಸೂರತ್ ಜನರು ಚೀನಾದ ಜತೆ ಸ್ಪರ್ಧೆ ನಡೆಸುತ್ತಿದ್ದಾರೆ\"', 'ಕಳ್ಳತನವಾಗಿದ್ದ 8 ಲಕ್ಷ ರೂ.', 'ಇದರ ಬಗ್ಗೆ ನಾನೂ ಸಾಕಷ್ಟು ಓದಿದ್ದೇನೆ.', 'ಮಾನವಕುಲವು ಸೈತಾನನ ಆಳಿಕೆಯ ಕೆಳಗೆ ಬಂದದ್ದು ಹೇಗೆ?', 'ನಂತರ ಪ್ರಧಾನಿ ಕೂಡ ಆಗುತ್ತೇನೆ.', 'ಭ್ರಷ್ಟಾಚಾರ ಏಕಿದೆ?', 'ಆ ಚಿತ್ರದ ಶೇ 90ರಷ್ಟು ಚಿತ್ರೀಕರಣವೂ ಈಗಾಗಲೇ ಮುಗಿದು ಹೋಗಿದೆ.', 'ವಿಶೇಷ ಕಾನೂನು', 'ಆಗ ಅರಸನು ಗಿತ್ತೀಯನಾದ ಇತ್ತೈಯನ್ನು ನೋಡಿ--ನೀನು ನಮ್ಮ ಸಂಗಡ ಬರುವದು ಯಾಕೆ? ನಿನ್ನ ಸ್ಥಳಕ್ಕೆ ಹಿಂದಿರುಗಿ ಹೋಗಿ ಅರಸನ ಸಂಗಡ ಇರು. ಯಾಕಂದರೆ ನೀನು ಸೆರೆಹಿಡಿಯಲ್ಪಟ್ಟವನಾದ ಅನ್ಯದೇಶದವನು.', 'ವಿಶ್ವ ಗೋ ಸಮ್ಮೇಳನದ ಅಂಗಳದಲ್ಲಿ ಏನೇನು ನಡೆದಿದೆ?', 'ಪ್ರಧಾನ ಮಂತ್ರಿ ನರೇಂದ್ರ ಮೋದಿ, ರಕ್ಷಣಾ ಸಚಿವ ರಾಜನಾಥ್ ಸಿಂಗ್ ಮತ್ತು ಕೇಂದ್ರ ಗೃಹ ಸಚಿವ ಅಮಿತ್ ಷಾ ಅವರು ಮಸೂದೆಯ ಬಗ್ಗೆ ಸಾರ್ವಜನಿಕ ಚರ್ಚೆ ಗೆ ಬರುವಂತೆ ಸಂಘ ಸವಾಲು ಹಾಕಿದೆ.', 'ಎಂಬುದು ಬಹಳ ದೀರ್ಘ ಕಾಲದಿಂದಲೂ ಚರ್ಚಿತವಾಗುತ್ತಿರುವ ವಿಷಯ.', 'ಇಲ್ಲಿಗೆ ಬರಲು ಬಸ್ ಸೌಕರ್ಯವೂ ಇಲ್ಲ.', 'ಆ ಪರಂಪರೆ ಯಾಕೆ ಮುನ್ನೆಲೆಗೆ ಬರಲಿಲ್ಲ?', 'ಭಾರತೀಯ ಸೇನೆ ಸೇರಲು ಮುಗಿಬೀಳುತ್ತಿರುವ ಕಾಶ್ಮೀರಿ ಯುವಕರು !', 'ಕುಗ್ರಾಮವಾದ ಈ ಹಳ್ಳಿಯಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳು', 'ರಾಜ್ಯದ ಬಜೆಟ್ ಯೇತರ ಸಾಲವು 2011-12ರಲ್ಲಿದ್ದ 1,853ಕೋಟಿ ರೂಪಾಯಿಯಿಂದ 2017-18ರಲ್ಲಿ 13,173 ಕ್ಕೆ ಏರಿಕಯಾಗಿದೆ.', 'ಮಳೆಗಾಲದಲ್ಲಿ ಕೊಳಚೆ ನೀರಿನೊಂದಿಗೆ ಮಳೆನೀರು ರಸ್ತೆಯಲ್ಲಿಯೇ ಮಡುಗಟ್ಟಿ ನಿಂತು ಸೊಳ್ಳೆಗಳ ಹೆಚ್ಚಳಕ್ಕೆ ಕಾರಣವಾಗುತ್ತಿದೆ.', 'ಯಾವುದೇ ಕಾರಣ ನೀಡದೆ ರಮೇಶ್ ಜಾರಕಿಹೊಳಿ ಮತ್ತು ಮಹೇಶ್ ಕಮಟಹಳ್ಳಿ ಗೈರು ಹಾಜರಾಗಿದ್ದಾರೆ ಎಂದು ಮಾಹಿತಿ ನೀಡಿದ್ದಾರೆ.', '\"ಹೇಗೆ \"\"ಗೂಳಿಕಾಳಗ\"\" ಒಂದು ಸಲಾಡ್ ತಯಾರು ಹೇಗೆ?\"', 'ಅದ್ರಲ್ಲೂ ಪ್ರಮುಖವಾಗಿ ದೇಶದ ಆರ್ಥಿಕತೆ ಪಾತಾಳಕ್ಕೆ ಕುಸಿದಿರೋದಕ್ಕೆ ಮೋದಿ ಸರ್ಕಾರವೇ ಸರ್ಕಾರ ಅಂತಾ ಟೀಕಿಸುತ್ತಿದ್ದಾರೆ.', 'ಅವರಿಗೆ ಕತ್ತರಿಸಿದ ಪಾರ್ಸ್ಲಿ ಮತ್ತು ತುಳಸಿ ಸೇರಿಸಿ.', 'ಮೊದಲ ಚಿತ್ರ ಶೂಟಿಂಗ್', 'ಅಪಘಾತದ ನಡೆದ ಕೂಡಲೇ ಮಾಹಿತಿ ಪಡೆದು ಸ್ಥಳಕ್ಕೆ ಆಗಮಿಸಿದ ಪೋಲಿಸರು ಗಾಯಗೊಂಡವರನ್ನು ಆಸ್ಪತ್ರೆ ದಾಖಲಿಸಿದ್ದಾರೆ.', 'ಆಕೆಯ ತಂದೆ ತನ್ನ ಮಕ್ಕಳಿಲ್ಲದ ಸೋದರಸಂಬಂಧಿ ಕುಂತಿಭೋಜನಿಗೆ ಕುಂತಿಯನ್ನು ಕೊಟ್ಟರು.', 'ಆಹಾರ ಬೇಯಿಸುವುದು ಹೇಗೆ', 'ಜೀವನಮಟ್ಟವು ದೇಶದಿಂದ ದೇಶಕ್ಕೆ ಬದಲಾಗುತ್ತದೆ ನಿಜ.', 'ಉದಾಹರಣೆಗೆ, ಖಗೋಳಶಾಸ್ತ್ರ.', 'ಇದೊಂದು ಉತ್ತಮ ಯೋಜನೆ.')]\n",
            "[(\"you don't know this.\", 'you are respected in society.', 'is this pic real?', '\"\"\"felicitations to all for the foundation laying of ram temple in ayodhya.\"', 'she looked stunningly beautiful in that', '{ -brand-name-nightly } blog', 'it was consistent.', 'case has been registered in malpe police station.', 'they still exist to this day.', 'breaking up isnt easy', 'students should abide by the rules.', 'we try to understand her well.', 'then the angel of yahweh commanded gad to tell david that david should go up, and raise an altar to yahweh in the threshing floor of ornan the jebusite.', 'for example, he forbids sexual immorality, idolatry, stealing, and drunkenness.', 'it is also a famous tourist destination', 'doctors and other persons concerned have been questioned.', 'but in politics.', 'to understand gods actions, we need to answer three questions: (1) who starts the war?', '14,000 crores.', 'what are those words?', 'shivakumars assumption of office as president of karnataka pradesh congress committee (kpcc).', 'now we were notified that we would receive the magazines in russian only.', 'they are found along coastlines around the world except antarcticas.', 'bill clinton went on to become president.', 'the culmination of the three stories towards the end. gives a new dimension to the film as a whole and concludes shuddhi.', 'while individuals may be allowed to die, god will never allow the extermination of his people as a whole.', '(the author is an educationist)', 'therefore i will not refrain my mouth. i will speak in the anguish of my spirit. i will complain in the bitterness of my soul.', 'are they hungry for knowledge?', 'the door did not open.'), ('ಈ ಬಗ್ಗೆ ನೀವಿನ್ನು ತಿಳಿದಿಲ್ಲ ಎ .', 'ಸಮಾಜದಲ್ಲಿ ಗೌರವವಿರುತ್ತದೆ.', 'ಈ ಫೋಟೋ ವಾಸ್ತವೋ, ಅಸಲಿಯೋ ?', \"'ಅಯೋಧ್ಯೆಯ ರಾಮ ದೇವಾಲಯದ ಅಡಿಪಾಯ ಹಾಕಿದ್ದಕ್ಕಾಗಿ ಎಲ್ಲರಿಗೂ ಶುಭಾಶಯಗಳು.\", 'ಅವಳು ಅಥವಾ ಆತ ನೋಡಲು ತುಂಬಾ ಸುಂದರ', 'ನೈಟ್ಲಿ ಬ್ಲಾಗ್', 'ಇದು ಸಂಕೇತವಾಗಿತ್ತು.', 'ಪ್ರಕರಣ ಮಳವಳ್ಳಿ ಪೊಲೀಸ್ ಠಾಣೆಯಲ್ಲಿ ದಾಖಲಾಗಿದೆ.', 'ಅಂತೆಯೇ ಅವರು ಇಂದಿಗೂ ಪ್ರಸ್ತುತವಾಗಿದ್ದಾರೆ.', 'ವೈರಾಗ್ಯ ಸುಲಭವಲ್ಲ', 'ವಿದ್ಯಾರ್ಥಿಗಳು ಕೂಡಾ ಶಿಸ್ತು ಪಾಲಿಸಬೇಕು.', '\"\"\" ಅವರೊಂದಿಗೆ ಅವನಿಗೆ ವಿವರವಾಗಿ ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಪ್ರಯತ್ನಿಸೋಣ.\"', 'ಆಗ ಕರ್ತನ ದೂತನು ಗಾದನಿಗೆ--ದಾವೀದನು ಹೋಗಿ ಯೆಬೂಸಿಯನಾದ ಒರ್ನಾನನ ಕಣದಲ್ಲಿ ಕರ್ತನಿಗೆ ಬಲಿಪೀಠವನ್ನು ಕಟ್ಟುವ ಹಾಗೆ ದಾವೀದನಿಗೆ ಹೇಳು ಅಂದನು.', 'ಬೈಬಲಿನಲ್ಲಿ ಯೆಹೋವನು ನಮಗೆ ಸ್ಪಷ್ಟ ಆಜ್ಞೆಗಳನ್ನು ಕೊಟ್ಟಿದ್ದಾನೆ.', 'ಇದೊಂದು ಬಹು ಜನಪ್ರಿಯವಾದ ವಿಹಾರ ತಾಣವಾಗಿದೆ', 'ಶಸ್ತ್ರ ಚಿಕಿತ್ಸೆ ಮಾಡಿದ ವೈದ್ಯರು ಮತ್ತು ಇತರ ಸಿಬ್ಬಂದಿಯನ್ನು ವಿಚಾರಣೆಗೆ ಒಳಪಡಿಸಿದ್ದಾರೆ.', 'ಆದರೆ ರಾಜಕೀಯದಲ್ಲಿರುತ್ತೇನೆ.', '( ನೆಹೆಮಿಾಯ 9: 17) ಇದನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳಲು ಈ ಮೂರು ಪ್ರಶ್ನೆಗಳಿಗೆ ಉತ್ತರ ತಿಳಿಯಿರಿ: (1) ಯುದ್ಧ ಆರಂಭಿಸುವವರು ಯಾರು?', '14,000 ಒಟ್ಟುಗೂಡಿತು.', 'ಈ ಶಬ್ದಗಳು ಯಾವುವು?', 'ಕೊನೆಗೂ ಡಿಕೆ ಶಿವಕುಮಾರ್ ಅವರನ್ನ ಕರ್ನಾಟಕ ಪ್ರದೇಶ ಕಾಂಗ್ರೆಸ್ (ಕೆಪಿಸಿಸಿ) ಅಧ್ಯಕ್ಷರನ್ನಾಗಿ ಆಯ್ಮೆ ಮಾಡಿದೆ.', 'ಪತ್ರಿಕೆಯು ನಿಷೇಧಿಸಲ್ಪಟ್ಟಿತ್ತು, ಆದರೆ ಇತರ ಸ್ಥಳಗಳಿಂದ ಗುಟ್ಟಾಗಿ ನಾವು ಪತ್ರಿಕೆಗಳನ್ನು ಪಡೆಯುತ್ತಿದ್ದೆವು.', 'ಅಂಟಾರ್ಟಿಕಾ ಖಂಡದ ಹೊರತುಪಡಿಸಿ ಅವು ವಿಶ್ವದಾದ್ಯಂತ ಕಂಡುಬರುತ್ತವೆ.', 'ಇವರ ನಂತರ ಬಿಲ್ ಕ್ಲಿಂಟನ್ ಅಧ್ಯಕ್ಷ ಪಟ್ಟ ಅಲಂಕರಿಸಿದ್ದರು.', 'ಕೊನೆಯಲ್ಲಿ ಮೂರು ಕಥೆಗಳ ಪರಾಕಾಷ್ಠೆ. ಒಟ್ಟಾರೆಯಾಗಿ ಚಿತ್ರಕ್ಕೆ ಹೊಸ ಆಯಾಮವನ್ನು ನೀಡುತ್ತದೆ ಮತ್ತು ಸುಧಿಯನ್ನು ಮುಕ್ತಾಯಗೊಳಿಸುತ್ತದೆ.', 'ಕೆಲವು ವ್ಯಕ್ತಿಗಳು ಸಾಯುವಂತೆ ಅನುಮತಿಸಲ್ಪಡುವುದಾದರೂ, ತನ್ನ ಜನರೆಲ್ಲರೂ ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಮೂಲರಾಗುವಂತೆ ದೇವರು ಎಂದಿಗೂ ಅನುಮತಿಸುವುದಿಲ್ಲ.', '(ಲೇಖಕರು ಶಿಕ್ಷಣತಜ್ಞರು)', 'ಆದದರಿಂದ ನಾನು ನನ್ನ ಬಾಯಿಯನ್ನು ಮುಚ್ಚು ವದಿಲ್ಲ. ನಾನು ಆತ್ಮ ವೇದನೆಯಿಂದ ಮಾತನಾಡು ವೆನು. ನನ್ನ ಮನೋವ್ಯಥೆಯಲ್ಲಿ ನಾನು ಗುಣುಗುಟ್ಟು ವೆನು.', 'ನಿಂಗೆ ಜ್ಞಾನದ ಹಸಿವು ನೀಗಸ್ತಾರೆ ?', 'ಅಂಗಳದ ಬಾಗಿಲು ಹಾಕಿರಲಿಲ್ಲ.')]\n",
            "[('i am non-vegetarian.', 'most of the government schools lack the teachers.', 'to the fans.', 'during the investigation, it was found that the 130 activists were in regular contact with the jmb leadership, he said.', 'it is 260 km away from mumbai city.', 'not everything happens as we expect.', 'what are you talking?', 'that young couple certainly started their marriage off on the best of foundations.', 'abbey waterfall flows from a height of 60 feet.', 'the treatment depends largely on the type and stage of the cancer.', 'i learned how to cook.', 'is coconut oil harmful?', 'may i answer?', 'what is love?', 'symptoms include rashes, diarrhea, vomiting, stomach cramps and bloating.', 'during 2015, they plan on introducing two new amg vehicles the c 63 s and gt s supercar', 'candidates belonging to st/sc, pwd category are exempted from paying any fee.', 'i love', 'you cant have everything always.', 'the death toll of coronavirus rose to 718 as 37 casualties were reported in 24 hours', 'his life was devoted to the service of the country.', 'he lost his father at an early age.', 'the vehicle is completely burnt.', 'but it was cancelled at the last moment.', 'many people are dead due to floods and extensive damage to property has occurred.', 'the scheme will be rolled out in the next couple of months.', 'what is not nice?', 'and he said, bring me a new cruse, and put salt therein. and they brought it to him.', 'the tvs entorq 125 will be up against the likes of honda activa, suzuki access and others', 'he handed over the documents for the 900 sq ft plot to nagar panchayat chairman zahir farooqui.'), ('ನಾನೇನು ಸಂಪೂರ್ಣ ಸಸ್ಯಾಹಾರಿ ಅಲ್ಲ.', 'ಬಹುತೇಕ ಸರ್ಕಾರಿ ಶಾಲೆಗಳಲ್ಲಿ ಮೂಲಭೂತ ಸೌಕರ್ಯಗಳೇ ಇಲ್ಲದಿರುವುದು ಕಂಡುಬಂದಿದೆ.', 'ಎಂಬ ಅಭಿಮಾನಿಗಳಿಗೆ ಕಾಡುತ್ತಿದೆ.', 'ಜೆಎಂಬಿ ನಾಯಕತ್ವದ ಜತೆ 130 ಕಾರ್ಯಕರ್ತರು ನಿಕಟ ಸಂಪರ್ಕದಲ್ಲಿರುವುದು ತನಿಖಾ ಹಂತದಲ್ಲಿ ಬೆಳಕಿಗೆ ಬಂದಿದೆ ಎಂದು ತಿಳಿಸಿದರು.', 'ಇದು ರಾಜ್ಯದ ರಾಜಧಾನಿಯಾದ ಮುಂಬೈನಿಂದ ಸುಮಾರು 260 ಕಿಮೀ ದೂರದಲ್ಲಿದೆ.', 'ಎಲ್ಲವೂ ನಾವು ಅಂದುಕೊಂಡಂತೆಯೇ ಆಗುವುದಿಲ್ಲ.', '\"\"\"ಏನೆಲ್ಲ ಮಾತಾಡತಾನೆ?\"', 'ಕೊನೆಯದಾಗಿ, ಈ ಘಟನೆಗಳು ಯೋಸೇಫ ಮರಿಯ ಇಬ್ಬರಿಗೂ ಪ್ರಾಮಾಣಿಕವೂ ಮುಕ್ತವೂ ಆದ ಸಂವಾದವು ಎಷ್ಟು ಮಹತ್ವ ಎಂಬ ವಿಷಯದಲ್ಲಿ ಹೆಚ್ಚನ್ನು ಕಲಿಸಿದ್ದಿರಬೇಕು.', '60 ಅಡಿ ಎತ್ತರದಿಂದ ನೀರು ಧರೆಗೆ ಧುಮ್ಮಿಕ್ಕುತ್ತದೆ.', 'ಚಿಕಿತ್ಸೆಯ ವಿಧಾನದ ಆಯ್ಕೆ ಹೆಚ್ಚಾಗಿ ರೋಗದ ಹಂತ ಮತ್ತು ಸ್ವರೂಪವನ್ನು ಅವಲಂಬಿಸಿರುತ್ತದೆ.', 'ಅಷ್ಟುಇಷ್ಟು ಅಡುಗೆ ಮಾಡುವುದನ್ನು ಕಲಿತುಬಿಟ್ಟೆ.', 'ಕಾರ್ನ್ ಎಣ್ಣೆ ಉಪಯುಕ್ತವಾಗಿದೆ?', '\"ಪ್ರಶ್ನೆ, \"\"ನಾನು ಉತ್ತರಿಸಬೇಕೇ?\"', 'ಪ್ರೀತಿ ಅಂದರೆ ಇದೇನಾ?', 'ರೋಗಲಕ್ಷಣಗಳು ಕ್ಯಾಂಪಿಂಗ್, ಭಾರಿ ಅವಧಿ, ಹೆಪ್ಪುಗಟ್ಟುವಿಕೆ, ಕೆಳ ಹೊಟ್ಟೆ ನೋವು, ಮತ್ತು ಉಬ್ಬುವುದು.', 'ಈ ಸಾಲಿಗೆ ಪ್ರಸಕ್ತ ವರ್ಷದಲ್ಲಿ ಎಎಂಜಿ ಸಿ 63 ಎಸ್ ಮತ್ತು ಜಿಟಿ ಎಸ್ ಸೂಪರ್ ಕಾರು ಸೇರ್ಪಡೆಯಾಗಲಿದೆ', 'ಎಸ್ಸಿ, ಎಸ್ಟಿ / ಪಿಡಬ್ಲ್ಯೂಡಿ ವಿಭಾಗಗಳಿಗೆ ಸೇರಿದ ಅಭ್ಯರ್ಥಿಗಳು ಅರ್ಜಿ ಪ್ರಕ್ರಿಯೆ ಶುಲ್ಕದಿಂದ ವಿನಾಯಿತಿ ಪಡೆದಿರುತ್ತಾರೆ.', 'ನಾನು ಪ್ರೀತಿಸಿದ್ದೇನೆ.', 'ಯಾವಾಗಲೂ ಎಲ್ಲಾ ಪದಾರ್ಥಗಳನ್ನು ಹೊಂದಿಲ್ಲ.', 'ದೇಶಾದ್ಯಂತ ಕರೋನಾ ಸೋಂಕಿತರ ಸಂಖ್ಯೆ 23 ಸಾವಿರಕ್ಕೂ ಹೆಚ್ಚಿದ್ದು, ಒಟ್ಟು ಈವರೆಗೆ 718 ಜನರು ಮೃತಪಟ್ಟಿದ್ದಾರೆ', 'ಅವರ ಜೀವನವನ್ನು ರಾಷ್ಟ್ರ ಸೇವೆಗೆ ಮಾತ್ರ ಮುಡಿಪಾಗಿಟ್ಟಿದ್ದರು ಎಂದು ತಿಳಿಸಿದರು.', 'ಬಾಲ್ಯದಲ್ಲೇ ತಂದೆಯನ್ನು ಕಳೆದುಕೊಂಡರು.', 'ಕಾರು ಸಂಪೂರ್ಣ ಬೆಂಕಿಗೆ ಆಹುತಿ ಆಗಿದೆ.', 'ಆದರೆ ಕೊನೇ ಕ್ಷಣದಲ್ಲಿ ತಮ್ಮ ನಿರ್ಧಾರದಿಂದ ಹಿಂದೆ ಸರಿದಿದ್ದಾರೆ.', 'ಪ್ರವಾಹದಿಂದ ಬಹಳಷ್ಟು ಆಸ್ತಿ ನಷ್ಟವಾಗಿದ್ದು, ಜನರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಮುಂದಿನ ಕೆಲವು ತಿಂಗಳಲ್ಲಿ ಈ ಯೋಜನೆಯನ್ನು ಪ್ರಕಟಿಸಲಾಗುವುದು.', 'ಚೆಂದ ಇಲ್ಲದಿದ್ದರೇನು?', 'ಅದಕ್ಕ ವನು--ನನ್ನ ಬಳಿಗೆ ಹೊಸ ಗಡಿಗೆಯನ್ನು ತಕ್ಕೊಂಡು ಬಂದು ಅದರಲ್ಲಿ ಉಪ್ಪು ಹಾಕಿರಿ ಅಂದನು. ಅವರು ಅದನ್ನು ಅವನ ಬಳಿಗೆ ತಕ್ಕೊಂಡು ಬಂದರು.', 'ಪ್ರಮುಖವಾಗಿಯೂ ಹೋಂಡಾ ಆಕ್ಟಿವಾ 125, ಸುಜುಕಿ ಆಕ್ಸೆಸ್ 125 ಹಾಗೂ ಮಹೀಂದ್ರ ಗಸ್ಟೊ 125 ಮಾದರಿಗಳಿಗೆ ಟಿವಿಎಸ್ ನೂತನ ಸ್ಕೂಟರ್ ಪ್ರತಿಸ್ಪರ್ಧಿಯಾಗಲಿದೆ', '900 ಚದರ ಅಡಿ ಜಾಗದ ಭೂ ದಾಖಲೆಗಳನ್ನು ಅವರು ನಗರ ಪಂಚಾಯತ್ ಅಧ್ಯಕ್ಷ ಜಹೀರ್ ಫಾರೂಕಿ ಅವರಿಗೆ ಹಸ್ತಾಂತರಿಸಿದರು.')]\n",
            "[('the engine is mated to a six-speed gearbox with a slip and assist clutch', 'this is what he has said.', 'but it is not taking off.', \"conflicting reports on amitabh bachchan's health confuse fans\", 'howsoever powerful.', 'the conversation thereafter went as follows:', 'what is the opposition doing?', 'best of luck to all team members.', 'but it is very expensive.', \"shah rukh khan's next film is titled sanki and will be directed by tamil director atlee.\", 'thats exactly what organizations are saying.', 'bike rider killed in motorcycle-tanker collision', 'students need not worry about it, he said.', 'there is no risk to anyone.', 'the matter had created uproar across the state.', 'parents are overjoyed to see the success of their children.', 'elections will be held soon.', 'to and fro vehicular traffic to the hill from uttanahalli road side has been completely banned.', 'do you leave?', 'define your goals and objectives.', \"what's up.\", 'how long will you take for completing the investigation?', 'there are two engine options.', 'the film in question has not yet received the certificate from censor board.', '\"even chhatrapati shivaji maharaj faced opposition from his own family,\"\" he said.\"', 'how to make a cake at home?', 'there was no harm to any passenger due to the fire.', 'they too moved their forces.', 'development has taken a hit in the state.', 'and this can be done by:'), ('ಈ ಎಂಜಿನ್ ಅನ್ನು 6- ಸ್ಪೀಡ್ ಗೇರ್ಬಾಕ್ಸ್ಗೆ ಅಸಿಸ್ಟ್ ಮತ್ತು ಸ್ಲಿಪ್ಪರ್ ಕ್ಲಚ್ನೊಂದಿಗೆ ಜೋಡಿಸಲಾಗಿದೆ', 'ಈ ಮೂಲಕ ತಾವೇನೆಂಬುದನ್ನು ತಿಳಿಸಿದ್ದಾರೆ.', 'ಆದರೆ, ಅದನ್ನು ಬಿಡುಗಡೆ ಮಾಡುತ್ತಿಲ್ಲ.', 'ಅಭಿಮಾನಿಗಳಲ್ಲಿ ಗೊಂದಲ ಸೃಷ್ಟಿಸಿದ ಅಮಿತಾಬ್ ಬಚ್ಚನ್ ಅನಾರೋಗ್ಯ ಸುದ್ದಿ', 'ಷ್ಟೇ ಬಲಶಾಲಿ ಆಗಿರಲಿ.', 'ಆನಂತರ ನಡೆದ ಸಂಭಾಷಣೆಯ ಸಾರಾಂಶ ಹೀಗಿದೆ.', 'ವಿರೋಧ ಪಕ್ಷದವರ ಕೆಲಸ ಏನಿದೆ?', 'ತಂಡದ ಸದಸ್ಯರಿಗೆಲ್ಲಾ ಅದೃಷ್ಟ ತಂದುಕೊಡುತ್ತವೆ.', 'ಆದರೆ ಆರ್ಥಿಕ ದೃಷ್ಟಿಯಿಂದ ಬಹಳ ದುಬಾರಿಯಾಗಿರುತ್ತದೆ.', 'ಶಾರುಖ್ ಖಾನ್ ಅವರ ಮುಂದಿನ ಚಲನಚಿತ್ರವನ್ನು ತಮಿಳು ನಿರ್ದೇಶಕ ಅಟ್ಲೀ ನಿರ್ದೇಶನ ಮಾಡಲಿದ್ದಾರೆ.', 'ಹಾಗೆಂದು ಆ ಸಂಸ್ಥೆಯೇ ಹೇಳಿಕೊಳ್ಳುತ್ತಿದೆ.', 'ಕಬ್ಬು ಸಾಗಿಸುತ್ತಿದ್ದ ಲಾರಿ ಹಾಗೂ ಬೈಕ್ ನಡುವೆ ಡಿಕ್ಕಿ : ಸವಾರ ಸಾವು', 'ಇದರ ಬಗ್ಗೆ ಯಾವುದೇ ಆತಂಕ ಬೇಡ ಎಂದು ವಿದ್ಯಾರ್ಥಿಗಳಿಗೆ ತಿಳಿಸಿದರು.', 'ಯಾರಿಗೂ ಅಂತಹ ಅಪಾಯಗಳಾಗಿಲ್ಲ ಎಂದರು.', 'ಈ ಸುದ್ದಿ ರಾಜ್ಯಾದ್ಯಂತ ಚರ್ಚೆಗೆ ಕಾರಣವಾಗಿತ್ತು.', 'ಪ್ರತಿ ಹೆತ್ತವರಿಗೂ ಅವರವರ ಮಕ್ಕಳ ಸಾಧನೆ ನೋಡೋ ಖುಷಿ ಇದ್ದೇ ಇರುತ್ತೆ.', 'ಚುನಾವಣೆ ಇನ್ನೇನು ಸಧ್ಯದಲ್ಲೇ ನಡೆಯಲಿದೆ.', 'ಉತ್ತನಹಳ್ಳಿ ರಸ್ತೆಯ ಕಡೆಯಿಂದ ಚಾಮುಂಡಿ ಬೆಟ್ಟಕ್ಕೆ ಬರುವ ಮತ್ತು ಹೋಗುವ ವಾಹನಗಳಿಗೆ ಸಂಚಾರವನ್ನು ಸಂಪೂರ್ಣವಾಗಿ ನಿರ್ಬಂಧಿಸಲಾಗಿದೆ.', 'ಬಿಡಲಿಕ್ಕೆ ಆಗುತ್ತಾ?', 'ನಿಮ್ಮ ಗುರಿ ಮತ್ತು ಉದ್ದೇಶಗಳನ್ನು ಗುರುತಿಸಿ.', 'ಏನು ಉರುಳೇ.', 'ತನಿಖೆಯನ್ನು ಪೂರ್ಣಗೊಳಿಸಲು ನಿಮಗೆ ಇನ್ನೂ ಎಷ್ಟು ಸಮಯ ಬೇಕು?', 'ಎರಡು ಎಂಜಿನ್ ಆಯ್ಕೆಗಳಿವೆ.', 'ಸೆನ್ಸಾರ್ ಮಂಡಳಿಯಿಂದ ಈ ಸಿನಿಮಾಗೆ ಇನ್ನೂ ಪ್ರಮಾಣಪತ್ರ ನೀಡಿಲ್ಲ.', 'ಛತ್ರಪತಿ ಶಿವಾಜಿ ಕೂಡ ತಮ್ಮದೇ ಕುಟುಂಬದಿಂದಲೇ ವಿರೋಧವನ್ನು ಎದುರಿಸಿದ್ದರು.', 'ಹೇಗೆ ಮನೆಯಲ್ಲಿ ಒಂದು ಕಾರ್ಟೂನ್ ರಚಿಸಲು?', 'ಬೆಂಕಿ ಅವಘಡದಲ್ಲಿ ಪ್ರಯಾಣಿಕರ ಪ್ರಾಣಕ್ಕೆ ಹಾನಿಯಾಗಿಲ್ಲ.', 'ತಮ್ಮ ಅಧಿಕಾರವನ್ನು ಚಲಾಯಿಸಿದ್ದರು ಕೂಡಾ.', 'ರಾಜ್ಯದಲ್ಲಿ ಅಭಿವೃದ್ಧಿ ಪರ್ವ ಬಂದಿದೆ.', 'ಮತ್ತು ನೀವು ಇದನ್ನು ಪರಿಹರಿಸಬಹುದು:')]\n",
            "[('a proposal has been made.', 'but the government is unmoved.', 'the programme was attended by students and teachers of the school.', 'they taught me much about the true holy father in heaven, jehovah god.', 'what we can do', 'the bjp is trying to gain a strong foothold in the state.', 'farmers are in distress all over the country.', 'but euphoria, relief, and achievement likewise provoke emotional tears in this case, tears of joy.', 'and to pass by you into macedonia, and to come again out of macedonia unto you, and of you to be brought on my way toward judaea.', 'in case of an attack, we need to respond swiftly to minimise the damage.', 'for unknown reasons.', 'community spread', 'which board?', 'there are 14 girls and 12 junior players.', 'preparing for the role', 'recipe: banana blossom salad', 'do not isolate yourself from your faithful christian brothers and sisters.', 'patient with leukaemia disease.', 'the total length of the highway road is 250 km.', 'the governors post is a constitutional post.', 'development and growth', 'it was even found among the ones he had chosen as apostles!', 'in their hands lies the future of india.', 'government high school', 'they stay in their own world.', 'you cant be regretful.', 'location: ranga rao road, near shankar mutt, shankarapura, near basavanagudi, bangalore', \"that's all humbug.\", 'they were all there.', 'he was keen to learn english also.'), ('ಪ್ರಸ್ತಾವನೆ ಸಲ್ಲಿಸಿತ್ತು.', 'ಆದರೆ ಈ ಸರ್ಕಾರ ಸದ್ಯ ಅತಂತ್ರದಲ್ಲಿದೆ.', 'ಈ ಕಾರ್ಯಕ್ರಮದಲ್ಲಿ ಶಿಕ್ಷಕ ವೃಂದ ಹಾಗೂ ವಿದ್ಯಾರ್ಥಿಗಳು ಭಾಗಿಯಾಗಿದ್ದರು.', 'ಅವರು ನನಗೆ ಸ್ವರ್ಗದಲ್ಲಿರುವ ನಿಜವಾದ ಪವಿತ್ರ ತಂದೆ ಯೆಹೋವ ದೇವರ ಬಗ್ಗೆ ಬಹಳಷ್ಟನ್ನು ಕಲಿಸಿದರು.', 'ನಾವೇನು ಮಾಡಬಹುದು ?', 'ರಾಜ್ಯದಲ್ಲಿ ಬಿಜೆಪಿ ಅಧಿಕಾರ ಪಡೆದುಕೊಳ್ಳಲು ಶಥಾಯ ಗಥಾಯ ಪ್ರಯತ್ನ ಮಾಡುತ್ತಿದೆ', 'ದೇಶಾದ್ಯಂತ ರೈತರು ಸಂಕಷ್ಟದಲ್ಲಿದ್ದಾರೆ.', 'ಭಾವನಾತ್ಮಕ ಕಣ್ಣೀರು ಉಕ್ಕಿ ಬರಲು ಅನೇಕ ಕಾರಣಗಳಿವೆ.', 'ತರುವಾಯ ನಿಮ್ಮ ಮಾರ್ಗವಾಗಿ ಮಕೆದೋನ್ಯಕ್ಕೆ ಹೋಗಿ ತಿರಿಗಿ ಮಕೆದೋನ್ಯದಿಂದ ನಿಮ್ಮ ಬಳಿಗೆ ಬಂದು ನಿಮ್ಮಿಂದ ಯೂದಾಯಕ್ಕೆ ಸಾಗಕಳುಹಿಸಲ್ಪಡುವಂತೆಯೂ ಯೋಚಿಸಿದ್ದೆನು.', 'ದಾಳಿಯ ಸಂದರ್ಭದಲ್ಲಿ ಹಾನಿಯನ್ನು ಕನಿಷ್ಠಗೊಳಿಸಲು ನಾವು ಚುರುಕಾಗಿ ಕಾರ್ಯಾಚರಿಸಬೇಕಾಗುತ್ತದೆ.', 'ಸ್ಪಷ್ಟೀಕರಿಸದ ಕಾರಣಗಳಿಗಾಗಿ', 'ಸಮುದಾಯ ಹರಡುವಿಕೆ', 'ಯಾರಿಗೆ ಯಾವ ಮಂಡಳಿ?', 'ಇವುಗಳಲ್ಲಿ 14 ಗಂಡು, 12 ಹೆಣ್ಣು ಚಿರತೆ ಸೇರಿವೆ.', 'ಪಾತ್ರಕ್ಕಾಗಿ ತಯಾರಿ', '\"ಪಾಕವಿಧಾನ: ಹೂಕೋಸು \"\"ಆಲೂಗಡ್ಡೆ\"\" ಸಲಾಡ್\"', 'ನಂಬಿಗಸ್ತ ಸಹೋದರ ಸಹೋದರಿಯರಿಂದ ನಿಮ್ಮನ್ನು ಪ್ರತ್ಯೇಕಿಸಿಕೊಳ್ಳಬೇಡಿ.', 'ತೀವ್ರತರವಾದ ಲ್ಯುಕೇಮಿಯಾ ಬಳಲುತ್ತಿರುವ ರೋಗಿಗಳು.', 'ರಸ್ತೆಯಲ್ಲಿ ಒಟ್ಟು ಅಗಲ - 250 ಮೀಟರ್.', 'ರಾಜ್ಯಪಾಲರದು ಸಂವಿಧಾನಾತ್ಮಕ ಹುದ್ದೆ.', 'ಅಭಿವೃದ್ಧಿ ಮತ್ತು ಅನಾರೋಗ್ಯ', 'ಯೇಸು ಸಾಯುವ ದಿನದ ವರೆಗೂ ಅವರು ಹೆಬ್ಬಯಕೆಯನ್ನು ತೋರ್ಪಡಿಸಿದರು.', 'ಇವರ ಆಟದ ಮೇಲೆ ಭಾರತದ ಭವಿಷ್ಯ ನಿಂತಿದೆ.', 'ಸಮಸ್ಯೆಗಳ ಆಗರ ಸರ್ಕಾರಿ ಪ್ರೌಢಶಾಲೆ', 'ಇವರು ತಮ್ಮದೇ ಆಗಿರುವ ಲೋಕದಲ್ಲಿ ವಿಹರಿಸುತ್ತಾ ಇರುವರು.', 'ನಿಮಗೆ ಕ್ಷಮಿಸುವ ಮನಸ್ಸಿಲ್ಲದಿರಬಹುದು.', 'ಸ್ಥಳ : ಜಕ್ಕಸಂದ್ರ, ಸರ್ಜಾಪುರ ರಸ್ತೆ, ಬೆಂಗಳೂರು', 'ಅದೆಲ್ಲ ಗಿಮಿಕ್ ಅಷ್ಟೆ.', 'ಅವರೆಲ್ಲರೂ ಇದ್ದರು.', 'ಅವರಿಗೆ ಇಂಗ್ಲಿಷ್ ಕಲಿಯುವುದಕ್ಕೆ ತುಂಬಾ ಆಸೆ ಇತ್ತು.')]\n"
          ]
        }
      ],
      "source": [
        "for batch_num, batch in enumerate(iterator):\n",
        "    print(batch)\n",
        "    if batch_num > 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnanjzqtzQi8"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],\n",
        "                                reduction='none')\n",
        "\n",
        "# When computing the loss, we are ignoring cases when the label is the padding token\n",
        "for params in transformer.parameters():\n",
        "    if params.dim() > 1:\n",
        "        nn.init.xavier_uniform_(params)\n",
        "\n",
        "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_saWU5QmVem2"
      },
      "outputs": [],
      "source": [
        "NEG_INFTY = -1e9\n",
        "\n",
        "def create_masks(eng_batch, kn_batch):\n",
        "    num_sentences = len(eng_batch)\n",
        "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
        "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
        "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
        "\n",
        "    for idx in range(num_sentences):\n",
        "      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])\n",
        "      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)\n",
        "      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)\n",
        "      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
        "      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
        "      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True\n",
        "\n",
        "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
        "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
        "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
        "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdgtTSKvwN9_"
      },
      "source": [
        "Modify mask such that the padding tokens cannot look ahead.\n",
        "In Encoder, tokens before it should be -1e9 while tokens after it should be -inf.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLcXI4wkMLck"
      },
      "source": [
        "Note the target mask starts with 2 rows of non masked items: https://github.com/SamLynnEvans/Transformer/blob/master/Beam.py#L55\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju59VDGLuOqf",
        "outputId": "a9546647-efd9-423c-ae01-535f34317f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Iteration 0 : 5.6788458824157715\n",
            "English: hes a scientist.\n",
            "Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.\n",
            "Kannada Prediction: ೄೄೄ೦ೄ7ೄೕೄೄೄೄ9೦೦ೕೄೄೄ೦೦ೄೄೄೄಜಜಜಜಖಖೄಜಜಜಜಜ೦ಖ\"ಖಳಖಖಂಂಖಂಜಜಂಜಽಽಽಘೄಽ5ಽಽಂಪಽ9ೄೠ5ಜಖಖೞಜೄಘೄಜಜಡೕಖಖನಜಥಥೃೕೕಥಥಥೞೞಖಖಖಖಖೄ\"\"ಜಖ\"ಖೄೄೄಖನೕೕೞಖಖಖೄನನ9ೕಎಖಖಖಖಘೄ\"\"\"ಖಖ\"ಖೊ\"ನಧ\"\"೧೭೭ಘ೧ೊಖಖಡ7ಖ?ೄೄಜಜ೬೬ಜನೄಥೄಖಖ\"ನನಳಳ೭ಒ\"\"\"ಒ೬ೊೊ\"ೄ\"\"೬ಪ5ಪಪ೬ಘಉ೨೬5ೠ೬ಖ\"\n",
            "Evaluation translation (should we go to the mall?) : ('ದದದದ<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 100 : 3.536039113998413\n",
            "English: she ate it.\n",
            "Kannada Translation: ಅವಳು ಅವನಿಗೆ ಊಟ ಹಾಕಿದಳೂ.\n",
            "Kannada Prediction: ದಸ್   ್  ್್್ \n",
            "Evaluation translation (should we go to the mall?) : ('ದದ್                       ಿ <END>',)\n",
            "-------------------------------------------\n",
            "Iteration 200 : 3.2449276447296143\n",
            "English: caste and religion were unknown.\n",
            "Kannada Translation: ಜಾತಿ, ಬೇಧ ಎಂಬುದೇ ಗೊತ್ತಿರಲಿಲ್ಲ.\n",
            "Kannada Prediction: ಮರರ್  ಗವ ್   ್   ರ    ್ ಿ್ ್ ್\n",
            "Evaluation translation (should we go to the mall?) : ('ಇದರ್  ನ ನ್ ನ ನ್ನ್ನ ನಿ ನಿ ನಿ ನಿದಿದಿ ನಿ ನಿಲಿಲಿದಿದಿದಿದಿದಿ.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 300 : 2.973187208175659\n",
            "English: seeing this, ruler was elated and told his son that the strength of the rabbit is due to the valour of the region's citizenry.\n",
            "Kannada Translation: ಇದನ್ನು ನೋಡಿ, ಆಡಳಿತಗಾರನು ಉತ್ಸಾಹದಿಂದ ಮತ್ತು ಮೊಲದ ಬಲವು ಪ್ರದೇಶದ ನಾಗರಿಕರ ಶೌರ್ಯದ ಕಾರಣ ಎಂದು ತನ್ನ ಮಗನಿಗೆ ತಿಳಿಸಿದನು.\n",
            "Kannada Prediction: ಅದ್ಿ ು ಕ್ ್  ಹ ್್ ್ಿ ್್ ಮ ್ ಿರು್ ರ್ಬ್್ ್ ಹ್ ್್ಸ್್್ ಹ್ ್್ ು್ಬ್ ಿು ್್ಸ್ ುರ್್ಮು ಿಿಬುದ್ ಬ್್ ್ಪಾಿ್ ಿ ದಿ ್ದ್ ು್ \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದ್ ಮ್ ಅ ಮ್ ಮ್ಲ್ಲ್ತ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ದ್ದು.<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 400 : 2.8862688541412354\n",
            "English: i also had such a feeling.\n",
            "Kannada Translation: ನನಗಂತೂ ಅಂಥ ಅನುಭೂತಿಯೇ ಆಗಿದ್ದು.\n",
            "Kannada Prediction: ನಾ್ೆದ  ನಲದಾಅಗ್ ಿ ುದಾ ನನಳ ್ತ್ \n",
            "Evaluation translation (should we go to the mall?) : ('ಇದ್ಲ್ ಅನ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ<END>',)\n",
            "-------------------------------------------\n",
            "Iteration 500 : 2.9380745887756348\n",
            "English: what if its too late?\n",
            "Kannada Translation: ದೀರ್ಘಕಾಲ ಇದ್ದರೆ ನಾಟ್ ಏನಾಗುತ್ತದೆ?\n",
            "Kannada Prediction: ಇುದ ುಲಾ್ರ್ಮ  ಲುು ಹ್ಗುಲಹತ್ದಳ ್ತಿುತ\n",
            "Evaluation translation (should we go to the mall?) : ('ಅದರು ಅವಾರಿ ಅದ ಅದ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ಲ್ತ್ತ್ತ್ತ್ಲ್ಲ್ಲ್ಲ್ತ್ತ್ಲ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ಲ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ತ್ಲ್ತ್ತ್ಲ್ದ್ದ್ತ್ಲ.<END>',)\n",
            "-------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "transformer.train()\n",
        "transformer.to(device)\n",
        "total_loss = 0\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    iterator = iter(train_loader)\n",
        "    for batch_num, batch in enumerate(iterator):\n",
        "        transformer.train()\n",
        "        eng_batch, kn_batch = batch\n",
        "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)\n",
        "        optim.zero_grad()\n",
        "        kn_predictions = transformer(eng_batch,\n",
        "                                     kn_batch,\n",
        "                                     encoder_self_attention_mask.to(device),\n",
        "                                     decoder_self_attention_mask.to(device),\n",
        "                                     decoder_cross_attention_mask.to(device),\n",
        "                                     enc_start_token=False,\n",
        "                                     enc_end_token=False,\n",
        "                                     dec_start_token=True,\n",
        "                                     dec_end_token=True)\n",
        "        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)\n",
        "        loss = criterian(\n",
        "            kn_predictions.view(-1, kn_vocab_size).to(device),\n",
        "            labels.view(-1).to(device)\n",
        "        ).to(device)\n",
        "        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)\n",
        "        loss = loss.sum() / valid_indicies.sum()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        #train_losses.append(loss.item())\n",
        "        if batch_num % 100 == 0:\n",
        "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
        "            print(f\"English: {eng_batch[0]}\")\n",
        "            print(f\"Kannada Translation: {kn_batch[0]}\")\n",
        "            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)\n",
        "            predicted_sentence = \"\"\n",
        "            for idx in kn_sentence_predicted:\n",
        "              if idx == kannada_to_index[END_TOKEN]:\n",
        "                break\n",
        "              predicted_sentence += index_to_kannada[idx.item()]\n",
        "            print(f\"Kannada Prediction: {predicted_sentence}\")\n",
        "\n",
        "\n",
        "            transformer.eval()\n",
        "            kn_sentence = (\"\",)\n",
        "            eng_sentence = (\"should we go to the mall?\",)\n",
        "            for word_counter in range(max_sequence_length):\n",
        "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "                predictions = transformer(eng_sentence,\n",
        "                                          kn_sentence,\n",
        "                                          encoder_self_attention_mask.to(device),\n",
        "                                          decoder_self_attention_mask.to(device),\n",
        "                                          decoder_cross_attention_mask.to(device),\n",
        "                                          enc_start_token=False,\n",
        "                                          enc_end_token=False,\n",
        "                                          dec_start_token=True,\n",
        "                                          dec_end_token=False)\n",
        "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
        "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "                next_token = index_to_kannada[next_token_index]\n",
        "                kn_sentence = (kn_sentence[0] + next_token, )\n",
        "                if next_token == END_TOKEN:\n",
        "                  break\n",
        "\n",
        "            print(f\"Evaluation translation (should we go to the mall?) : {kn_sentence}\")\n",
        "            print(\"-------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nosVPGVijId"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOQe-juylBiJ"
      },
      "outputs": [],
      "source": [
        "transformer.eval()\n",
        "def translate(eng_sentence):\n",
        "  eng_sentence = (eng_sentence,)\n",
        "  kn_sentence = (\"\",)\n",
        "  for word_counter in range(max_sequence_length):\n",
        "    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)\n",
        "    predictions = transformer(eng_sentence,\n",
        "                              kn_sentence,\n",
        "                              encoder_self_attention_mask.to(device),\n",
        "                              decoder_self_attention_mask.to(device),\n",
        "                              decoder_cross_attention_mask.to(device),\n",
        "                              enc_start_token=False,\n",
        "                              enc_end_token=False,\n",
        "                              dec_start_token=True,\n",
        "                              dec_end_token=False)\n",
        "    next_token_prob_distribution = predictions[0][word_counter]\n",
        "    next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
        "    next_token = index_to_kannada[next_token_index]\n",
        "    kn_sentence = (kn_sentence[0] + next_token, )\n",
        "    if next_token == END_TOKEN:\n",
        "      break\n",
        "  return kn_sentence[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVH_YsxlK6q",
        "outputId": "83c47f99-53c0-4c2d-c26a-aaa426f50563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರ ಬಗ್ಗೆ ಏನು ಮಾಡಬೇಕು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"what should we do when the day starts?\")\n",
        "print(translation)\n",
        "#ದಿನ ಪ್ರಾರಂಭವಾದಾಗ ನಾವು ಏನು ಮಾಡಬೇಕು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9yfawBnul0W",
        "outputId": "d9e6e6b7-683b-45f9-f013-c53c31038306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಹೇಗೆ ಇದು ಹೇಗೆ ಹೇಗೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"how is this the truth?\")\n",
        "print(translation)\n",
        "#ಇದು ಹೇಗೆ ಸತ್ಯ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpdYBk5-urcQ",
        "outputId": "ca7249c5-efda-4f41-f052-ecef9691be82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರಿಂದ ಮೂಲಕ ಸಂಬಂಧಿಸಿದ ಮೇಲೆ ಮಾಡಿದ್ದಾರೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"the world is a large place with different people\")\n",
        "print(translation)\n",
        "#ಪ್ರಪಂಚವು ವಿಭಿನ್ನ ಜನರೊಂದಿಗೆ ದೊಡ್ಡ ಸ್ಥಳವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni9e2UYUuxi3",
        "outputId": "b93968e6-3f12-4794-b277-3a3821af221e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕುಟುಂಬದ ಹೆಸರು<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"my name is ajay\")\n",
        "print(translation)\n",
        "#ನನ್ನ ಹೆಸರು ಅಜಯ್"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJuJKHqFldW3",
        "outputId": "71aa2c6c-ec77-4b02-d39b-bd724012fb53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಅಂತರ ಸಂಗತಿ ನಾನು ಕೊಡುವುದಿಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i cannot stand this smell\")\n",
        "print(translation)\n",
        "#ನಾನು ಈ ವಾಸನೆಯನ್ನು ಸಹಿಸುವುದಿಲ್ಲ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxHC4Lirlfu8",
        "outputId": "5a3ca401-abac-41af-d9db-99fb7a57fe00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಯಾವುದೇ ಕಾರಣಗಳು ಇಲ್ಲ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"noodles are the best\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLVOSI0Oli16",
        "outputId": "9f9445a1-2802-4688-900c-d7ecf2bd5c35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಏಕೆ ಕಾರಣ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why care about this?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MStWCoAt0Ixp"
      },
      "source": [
        "This translated pretty well : \"What is the reason. Why\" without punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB6TEJfGlkRT",
        "outputId": "adb465d5-b0ed-4be4-9251-62c079f49491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದು ಅತ್ಯಂತ ಹೊರತಾಗಿದೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"this is the best thing ever\")\n",
        "print(translation)\n",
        "# ಇದು ಎಂದೆಂದಿಗೂ ಉತ್ತಮವಾಗಿದೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxsUjSybxYkh"
      },
      "source": [
        "The translation : \"This is very unusual\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQwDbuWBlmmA",
        "outputId": "7ae0e2c0-02c0-4c74-bc47-26b67da55a06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ಕೇಳಿದ್ದೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am here\")\n",
        "print(translation)\n",
        "# ನಾನು ಇಲ್ಲಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmyZ2-I6x0Yf"
      },
      "source": [
        "Translation: \"I have heard\".\n",
        "This is why word based translator may perform better than character translator. This is actually very good at optimizing the objective of the current transformer even though the translation is off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ifeV4bGluIj",
        "outputId": "6bce922d-d0db-432c-e6c6-97d482f1dea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಕ್ಯಾನ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಕ್ಲಿಕ್ ಮಾಡಿ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"click this\")\n",
        "print(translation)\n",
        "# ಇದನ್ನು ಕ್ಲಿಕ್ ಮಾಡಿ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RB5DUBEl1kD",
        "outputId": "9109e504-8b9e-45dc-e13c-e878b3741e4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಎಲ್ಲಿ ಎಲ್ಲಿ ಎಲ್ಲಿ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"where is the mall?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdeJ9CvMn5LM",
        "outputId": "044b5dac-29a9-4b60-e66a-4e10739a9756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಏನು ಮಾಡಬೇಕು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"what should we do?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoikFnov1rj-"
      },
      "source": [
        "This is correct; but it absolutely fumbles on the next one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GFeyzrg1fIZ",
        "outputId": "2b4dfb04-def2-4725-9e76-5476bf85e2ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅದನ್ನು ಮಾಡಿದ ಮೇಲೆ ಮಾಡಿದ ಮಾಡಿದ್ದಾರೆ<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"today, what should we do\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0upygLS-sXcO",
        "outputId": "53a62435-ab16-4d4c-8a9f-0bf07af2a501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅದರ ಮೇಲೆ ಏಕೆ ಮಾಡಿದ್ದಾರೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why did they activate?\")\n",
        "print(translation)\n",
        "# ಅವರು ಏಕೆ ಸಕ್ರಿಯಗೊಳಿಸಿದರು?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSrsqEGmtcl2",
        "outputId": "1b8b8faa-5370-426a-f2f2-fe852696bf7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಅವರು ಏಕೆ ಅವರು ಏಕೆ ಮಾಡಿದ್ದಾರೆ?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"why did they do this?\")\n",
        "print(translation)\n",
        "# ಅವರು ಇದನ್ನು ಏಕೆ ಮಾಡಿದರು?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7ISM5rd3BLJ"
      },
      "source": [
        "That turned out well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjTcH2HFtyld",
        "outputId": "9dea5498-f139-4cbd-ee8c-6108e1b92fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ನಾನು ನಾನು ಕೊಡುತ್ತೇನೆ.<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"i am well.\")\n",
        "print(translation)\n",
        "# ನಾನು ಆರಾಮವಾಗಿದ್ದೇನೆ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cP0YX2g74eP7"
      },
      "source": [
        "Translation: \"I will give you something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pGdN13kt5Br",
        "outputId": "240256c5-f594-41b0-8218-2c70a22a156f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ಇದರ ಬಗ್ಗೆ ಏನು?<END>\n"
          ]
        }
      ],
      "source": [
        "translation = translate(\"whats the word on the street?\")\n",
        "print(translation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFYZ6pOe4o-X"
      },
      "source": [
        "Kind of close semantically. Translation is something like: \"What is this about\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzrOcNUk1-e5"
      },
      "source": [
        "## Insights\n",
        "\n",
        "- When training, we can treat every alphabet as a single unit instead of splitting it into it's corresponding parts to preserve meaning. For example, ಮಾ should be 1 unit when comuting a loss. It should not be decomposed into ಮ + ఆ\n",
        "- Using word-based or BPE based tokenizations may help mitigate (1). Also, we will get valid word (or BPE) units if we do so.\n",
        "- Make sure the training set has a large variety of sentences that are not just about one topic like \"work\" and \"government\"\n",
        "- Increase the number of encoder / decoder units for better translations. It was set to the minimum of 1 of each unit here.\n",
        "- Create a translator with a language you understand ideally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd6_k0Uu5V7f"
      },
      "source": [
        "Overall, this model definately learned something. And you can use other languages instead of this kannada language and might see better luck"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}